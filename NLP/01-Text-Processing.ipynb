{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a38715e-cbcf-41e1-b428-0fdd0407f69f",
   "metadata": {},
   "source": [
    "# What is NLP?\n",
    "\n",
    "NLP = Teaching computers to understand and work with human language (like English, Hindi, etc).\n",
    "\n",
    "\n",
    "### Why is it needed?\n",
    "Because computers don‚Äôt understand human language naturally. They understand 0s and 1s ‚Äî so we use NLP to help them make sense of our messy, emotional, and context-heavy language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c480f8b0-9d2d-49ce-a48f-c16bece44333",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae755d64-31d1-4ff9-8777-392c64cb0207",
   "metadata": {},
   "source": [
    "### 1. Text Preprocessing in NLP\n",
    "\n",
    "Text data is messy. Before you can train a model, you must clean and standardize it.\n",
    "Let‚Äôs go through all the common preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a868cc1-a3b0-471d-ab30-777e3743d149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/abdullahshaikh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/abdullahshaikh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abdullahshaikh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/abdullahshaikh/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/abdullahshaikh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1e5102-88b0-4ab7-867b-16ecf89e81cd",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "### üëâ What is it?\n",
    "Breaking a sentence into smaller parts (words or sentences).\n",
    "These parts are called tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df3647a-433b-4c91-ba83-c78c92b937fe",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "\"I love machine learning\"\n",
    "\n",
    "After word tokenization ‚Üí\n",
    "\n",
    "`[\"I\", \"love\", \"machine\", \"learning\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2a5d030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Abdullah this side.', \"Hope you're doing\\ngood and in pink of health.\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### paragraph ---> Sentence\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "corpus = \"\"\"Hello Abdullah this side. Hope you're doing\n",
    "good and in pink of health.\n",
    "\"\"\"\n",
    "\n",
    "sent_tokenize(corpus) # works only on punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe9970c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Abdullah',\n",
       " 'this',\n",
       " 'side',\n",
       " '.',\n",
       " 'Hope',\n",
       " 'you',\n",
       " \"'re\",\n",
       " 'doing',\n",
       " 'good',\n",
       " 'and',\n",
       " 'in',\n",
       " 'pink',\n",
       " 'of',\n",
       " 'health',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### paragraph ---> words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e3684f7-fdd6-4169-846c-a3b7409cfb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Abdullah',\n",
       " 'this',\n",
       " 'side',\n",
       " '.',\n",
       " 'Hope',\n",
       " 'you',\n",
       " \"'\",\n",
       " 're',\n",
       " 'doing',\n",
       " 'good',\n",
       " 'and',\n",
       " 'in',\n",
       " 'pink',\n",
       " 'of',\n",
       " 'health',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd522122-27ba-4a08-82b5-93fd61090e38",
   "metadata": {},
   "source": [
    "### Step 2: Stopword Removal\n",
    "\n",
    "#### What are stopwords?\n",
    "Stopwords are common words in a language that don‚Äôt add much meaning for analysis.\n",
    "\n",
    "Examples of stopwords in English:\n",
    "\n",
    "`[is, the, a, an, and, I, you, of, to, in]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d12a441-b91b-46e2-ab49-072228f57fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['learning', 'natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"I am learning natural language processing\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "filtered = [w for w in words if w.lower() not in stopwords.words('english')]\n",
    "print(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e6130-dcb0-4ff9-a023-8bdc0c72d063",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4451613-2a35-46e5-ba6e-88d0d0561b17",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "\n",
    "Both are used to reduce a word to its root form, but they do it differently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1664c97a-853b-4cd4-87dd-ae3d6233aa4b",
   "metadata": {},
   "source": [
    "### Stemming ‚Äì (Rough Cut ‚úÇÔ∏è)\n",
    "Reduce a word to its <b>base/root</b> by chopping off suffixes, regardless of whether it becomes a meaningful word or not.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69e5a89-2c62-4be5-bbce-1c5a5ed525f1",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "`\"playing\", \"played\", \"plays\" ‚Üí \"play\"`\n",
    "\n",
    "\n",
    "\n",
    "`\"studies\", \"studying\" ‚Üí \"studi\"`\n",
    "\n",
    "\n",
    "Common Stemmers in Python:\n",
    "\n",
    "- PorterStemmer ‚Äì basic and common\n",
    "\n",
    "- LancasterStemmer ‚Äì more aggressive\n",
    "\n",
    "- SnowballStemmer ‚Äì smarter than Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af7344d1-4e15-486e-ba24-f3a21564feb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PorterStemmer Results:\n",
      "playing ‚Üí play\n",
      "flies ‚Üí fli\n",
      "happiness ‚Üí happi\n",
      "maximum ‚Üí maximum\n",
      "emotion ‚Üí emot\n",
      "studies ‚Üí studi\n",
      "crying ‚Üí cri\n",
      "nationalism ‚Üí nation\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"playing\", \"flies\", \"happiness\", \"maximum\", \"emotion\", \"studies\", \"crying\", \"nationalism\"]\n",
    "\n",
    "print(\"PorterStemmer Results:\")\n",
    "for word in words:\n",
    "    print(f\"{word} ‚Üí {stemmer.stem(word)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fb0be09-036b-47fe-aeb8-0f8ba55e4dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LancasterStemmer Results:\n",
      "playing ‚Üí play\n",
      "flies ‚Üí fli\n",
      "happiness ‚Üí happy\n",
      "maximum ‚Üí maxim\n",
      "emotion ‚Üí emot\n",
      "studies ‚Üí study\n",
      "crying ‚Üí cry\n",
      "nationalism ‚Üí nat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "words = [\"playing\", \"flies\", \"happiness\", \"maximum\", \"emotion\", \"studies\", \"crying\", \"nationalism\"]\n",
    "\n",
    "print(\"LancasterStemmer Results:\")\n",
    "for word in words:\n",
    "    print(f\"{word} ‚Üí {stemmer.stem(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18f0525d-9147-4ed6-966a-307585bcebc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play\n",
      "fli\n",
      "univers\n",
      "studi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "print(stemmer.stem(\"playing\"))       # play\n",
    "print(stemmer.stem(\"flies\"))         # fli\n",
    "print(stemmer.stem(\"universities\"))  # univers\n",
    "print(stemmer.stem(\"studying\"))      # studi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45d4e12b-7555-4555-b883-73e9291808ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SnowballStemmer Results:\n",
      "playing ‚Üí play\n",
      "flies ‚Üí fli\n",
      "happiness ‚Üí happi\n",
      "maximum ‚Üí maximum\n",
      "emotion ‚Üí emot\n",
      "studies ‚Üí studi\n",
      "crying ‚Üí cri\n",
      "nationalism ‚Üí nation\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "words = [\"playing\", \"flies\", \"happiness\", \"maximum\", \"emotion\", \"studies\", \"crying\", \"nationalism\"]\n",
    "\n",
    "print(\"SnowballStemmer Results:\")\n",
    "for word in words:\n",
    "    print(f\"{word} ‚Üí {stemmer.stem(word)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d47850-92de-4f58-ab8e-561ff9088497",
   "metadata": {},
   "source": [
    "## What is Lemmatization?\n",
    "\n",
    "Lemmatization is the process of converting a word to its dictionary <b>base form (called the lemma) using grammar rules and context.</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b0baa-79c2-4bd1-b442-14f1e30b5889",
   "metadata": {},
   "source": [
    "Real-Life Example:\n",
    "\n",
    "Original sentence:\n",
    "\n",
    "`\"The children were playing and studies were ongoing.\"`\n",
    "\n",
    "With stemming:\n",
    "\n",
    "`[\"The\", \"children\", \"were\", \"play\", \"and\", \"studi\", \"were\", \"ongo\"]`\n",
    "\n",
    "With lemmatization:\n",
    "\n",
    "`[\"The\", \"child\", \"be\", \"play\", \"and\", \"study\", \"be\", \"ongoing\"]`\n",
    "\n",
    "Much cleaner and meaningful, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2d4fc08-08c8-4235-bdfa-04012f78b6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ‚Üí The\n",
      "children ‚Üí child\n",
      "were ‚Üí were\n",
      "playing ‚Üí playing\n",
      "and ‚Üí and\n",
      "studies ‚Üí study\n",
      "were ‚Üí were\n",
      "ongoing ‚Üí ongoing\n"
     ]
    }
   ],
   "source": [
    "words = ['studies', \"children\", \"were\", \"playing\", \"and\", \"studies\", \"were\", \"ongoing\"]\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for word in words:\n",
    "    print(f\"{word} ‚Üí {lemmatizer.lemmatize(word)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4266356-1585-494f-8ddb-46bff5bb6df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "# POS - Part Of Speech\n",
    "\n",
    "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))  # Output: \"run\" (verb)\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\")) # adverb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886af6ec-515b-47b3-8395-08dfd1b32c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "studyenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
